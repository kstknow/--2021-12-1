{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11.텍스트 전처리-Colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- 텍스트 전처리"
      ],
      "metadata": {
        "id": "1B6wrS5ymHac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 토큰화\n",
        "1. 말뭉치가 있다. 문장으로 단어로 형태소까지 나누는 것을 토큰화\n",
        "2. 정체\n",
        "3.정규화"
      ],
      "metadata": {
        "id": "Zhmsnj5Zml5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYQLewAMoNvu",
        "outputId": "30ac967d-e1b8-4d23-c33d-f39e1cd05c0d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nriKQpW5p6Tq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) 단어 토큰화\n",
        "- 토큰의 기준을 단어로 하는 경우, 단어 토큰화라고 합니다\n",
        "- 작업은 단순히 구두점이나 특수문자를 전부 제거하는 정제 작업을 수행하는 것만으로 해결 되지 않는다."
      ],
      "metadata": {
        "id": "1SkQ9BdDoYvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\""
      ],
      "metadata": {
        "id": "YKe60LmRpU4O"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P-qsU9jpe2-",
        "outputId": "90ad61bf-bb2b-4fc5-e466-dc74e802e2d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 단위로 도출"
      ],
      "metadata": {
        "id": "OqObaPdQsYpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "wpt = WordPunctTokenizer()\n",
        "print(wpt.tokenize(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_elnTALQp75k",
        "outputId": "7c4a3598-45a6-49c8-efd8-c74265bb0b9f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
      ],
      "metadata": {
        "id": "MbwYROt_qHZT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_to_word_sequence(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUhCf1wFqih0",
        "outputId": "df189417-0bcf-43bf-ecc1-7be0134b7af0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tok = TreebankWordTokenizer()\n",
        "print(tok.tokenize(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7UcyKdBqrqE",
        "outputId": "047b6da3-1be4-4349-819f-dda2f3a3b1ab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) 문장 토큰화"
      ],
      "metadata": {
        "id": "CQs452ovrBvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize  # 문장을 구분해주는 것\n",
        "text ='His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.'\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtMiktOGrrDE",
        "outputId": "39d0b64a-db6b-47ff-f5d2-8c4ae9fcdeb2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 문장 단위로 구분되어 지는 리스트가 생김"
      ],
      "metadata": {
        "id": "CgMDHneRr5_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sent_tokenize(text):\n",
        "    print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEIqSOuksb39",
        "outputId": "80799c1c-e32e-4965-db0d-ce4d26d6cbec"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['His', 'barber', 'kept', 'his', 'word', '.']\n",
            "['But', 'keeping', 'such', 'a', 'huge', 'secret', 'to', 'himself', 'was', 'driving', 'him', 'crazy', '.']\n",
            "['Finally', ',', 'the', 'barber', 'went', 'up', 'a', 'mountain', 'and', 'almost', 'to', 'the', 'edge', 'of', 'a', 'cliff', '.']\n",
            "['He', 'dug', 'a', 'hole', 'in', 'the', 'midst', 'of', 'some', 'reeds', '.']\n",
            "['He', 'looked', 'about', ',', 'to', 'make', 'sure', 'no', 'one', 'was', 'near', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\"\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEy_cgBqskYa",
        "outputId": "d02a8db5-56ef-48e6-dadd-de55f9b94c3c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 한글 문장 토큰화 "
      ],
      "metadata": {
        "id": "11ZtbPvvtANr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KSS(korean Sentence Splitter) 설치\n",
        "!pip install kss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbdV4klEtJ3c",
        "outputId": "800cce77-fef8-4810-d35e-281a7d5fc896"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kss\n",
            "  Downloading kss-3.3.1.1.tar.gz (42.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.4 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 52.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kss, emoji\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-3.3.1.1-py3-none-any.whl size=42449239 sha256=6bd443915a59b04d75073ea10918f075c3a823535907afdd671eb4086839cf9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/9d/1d/52871154eff5273abb86b96f4f984c1cd67c5bde64239b060a\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=0d0d98a9ad627a36266f78380854ec5c27b5709157c548584761e0afecb8226f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "Successfully built kss emoji\n",
            "Installing collected packages: emoji, kss\n",
            "Successfully installed emoji-1.6.1 kss-3.3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KSS(korean Sentence Splitter) 설치tl 메세지를 보고 싶지 않을때\n",
        "!pip install kss > /dev/null "
      ],
      "metadata": {
        "id": "8-g02djBtVFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 리눅스 창에서 cmd처럼 쓰는 것은 shell\n",
        "# !ls -l : 파일 내용 들이 나온다.\n",
        "# 명령어를 !써서 한다 코랩같은 리눅스 가능\n",
        "# 프로그램 설치 pip install module pakage\n",
        "# 한글 쓸라면 한글 폰트 설치 apt-get으로 명령해야됨\n",
        "# 파일 업로드 - 시간이 오래 걸림\n",
        "# 파일 업로드 할때는 -> googl Drive -> colab에서 사용하게 조치( 빠른 시간)\n",
        "\n",
        "# Linux(unix) 명령어 - 알아둘 필요가 있음\n",
        "# Mac Terminal"
      ],
      "metadata": {
        "id": "PaICJe31t5kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kss\n",
        "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'"
      ],
      "metadata": {
        "id": "5ewVelYux6Fz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(kss.split_sentences(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5kAlEU-x7p0",
        "outputId": "e8db35fb-b0ac-41ff-ce7f-acf949ebce16"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Korean Sentence Splitter]: Initializing Pynori...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "한구어는 어절이 독립적인 단어로 구성되는 것이 아니라 조사등의 무언가가 붙어 있는 경우 많아서 이를 전부 분리 해줘야 한다는 의미입니다.\n",
        "한국어 토큰화에서는 형태소란 개념을 반드시 이해\n",
        "형태소란 뜻을 가진 가장 작은 말의 단위를 말합니다. 이 형태소에 는 두가지 형태소\n",
        "자립 형태소: 접사, 어미 조사와 상과없이 자립하여 사용할 수 있는 형태소\n",
        "의존 형태소: 다른 형태소와 결합하여 사용되는 형태소, 접사"
      ],
      "metadata": {
        "id": "s2K7iDbpyIxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 품사태깅\n",
        "- 단어는 표기는 같지만 품사에 따라서 단어의 의미가 달라지기도 합니다. 예를 들어서 영어 단어 fly는 동사로는 날다라는 의미를 갖지만,명사로서는 파리라는 의미를 가진다.\n"
      ],
      "metadata": {
        "id": "K19Krc4n0oQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "- 3) 품사(POS : Part of speech)태깅"
      ],
      "metadata": {
        "id": "v2k4QG0H07nP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "earZmVC61GAR",
        "outputId": "3c15e74b-4fc9-4fb7-c0ee-eb547cd2f3a6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T1-qin61hiu",
        "outputId": "c65307c7-4c58-4809-a28e-1ec87806d007"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "pos_tag(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaJjc7xl1Giz",
        "outputId": "bba19424-ee33-47dd-9b84-83309b32003b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('am', 'VBP'),\n",
              " ('actively', 'RB'),\n",
              " ('looking', 'VBG'),\n",
              " ('for', 'IN'),\n",
              " ('Ph.D.', 'NNP'),\n",
              " ('students', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('and', 'CC'),\n",
              " ('you', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('a', 'DT'),\n",
              " ('Ph.D.', 'NNP'),\n",
              " ('student', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Penn Treebank POG Tags에서 PRP는 인칭 대명사, VBP는 동사, RB는 부사, VBG는 현재부사, IN은 전치사, NNP는 고유 명사, NNS는 복수형 명사, CC는 접속사, DT는 관사를 의미합니다."
      ],
      "metadata": {
        "id": "h61aFcpX3Imr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어 처리를 위해서는 KoNLPy(코엔엘파이)"
      ],
      "metadata": {
        "id": "yPOrjP5F3JH5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 한글(KoNLPy: 코엔엘파이)"
      ],
      "metadata": {
        "id": "QAtE_8At3Qbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (KoNLPy: 코엔엘파이) 설치\n",
        "!pip install Konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "906mI0Em3bio",
        "outputId": "0426f6dd-df92-49f5-fe50-1080690b3532"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from Konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from Konlpy) (1.19.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 51.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->Konlpy) (3.10.0.2)\n",
            "Installing collected packages: JPype1, Konlpy\n",
            "Successfully installed JPype1-1.3.0 Konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Okt(open korean Text)"
      ],
      "metadata": {
        "id": "WGfHOMsh3kpA"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
        "okt.morphs(text) # 형태소 분석한 결과! "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TOPW7sD4Spg",
        "outputId": "a8bef378-ce2b-4376-a0b2-49e843e9885e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "okt.morphs(text, stem = True) # 동사의 원형이 나오게끔하는것 STEM = true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDDc8I1U4ZSY",
        "outputId": "a3cb2cd0-0cca-4f54-82df-0734d31e9473"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가보다']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 품사 부착\n",
        "okt.pos(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyVzbtCI4vDY",
        "outputId": "492deac6-c16c-424e-8173-a7e412ed8b24"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('열심히', 'Adverb'),\n",
              " ('코딩', 'Noun'),\n",
              " ('한', 'Josa'),\n",
              " ('당신', 'Noun'),\n",
              " (',', 'Punctuation'),\n",
              " ('연휴', 'Noun'),\n",
              " ('에는', 'Josa'),\n",
              " ('여행', 'Noun'),\n",
              " ('을', 'Josa'),\n",
              " ('가봐요', 'Verb')]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 명사 추출\n",
        "okt.nouns(text) # word cloud같은거 할때 필요함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P5oraTA5Ikg",
        "outputId": "e9e301a2-20a4-4f53-d885-6069ee0609ed"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['코딩', '당신', '연휴', '여행']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 꼬꼬마"
      ],
      "metadata": {
        "id": "vYT3i3Zj5ZKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma # 클래스 여서 첫글자는 대문자\n",
        "kkma = Kkma()\n",
        "kkma.morphs(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCWXIf9p56Fh",
        "outputId": "866c611a-1eeb-412b-92a5-4b3b64adca3e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kkma.pos(text) # 코드가 달라짐 확인!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEt203mZ6Hik",
        "outputId": "c0afcf40-ea3b-4ab0-b0fd-ee658539ea8e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('열심히', 'MAG'),\n",
              " ('코딩', 'NNG'),\n",
              " ('하', 'XSV'),\n",
              " ('ㄴ', 'ETD'),\n",
              " ('당신', 'NP'),\n",
              " (',', 'SP'),\n",
              " ('연휴', 'NNG'),\n",
              " ('에', 'JKM'),\n",
              " ('는', 'JX'),\n",
              " ('여행', 'NNG'),\n",
              " ('을', 'JKO'),\n",
              " ('가보', 'VV'),\n",
              " ('아요', 'EFN')]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kkma.nouns(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFCjAsxk6V2k",
        "outputId": "f340b93f-6496-4967-dac9-763982f4dbe6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['코딩', '당신', '연휴', '여행']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nLdDS7ln6a13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 정제: 갖고 잇는 코퍼스로부터 노이즈 데이터를 제거한다.\n",
        "# 정규화: 표현방법이 다른 단어들을 통합시켜서 같은 단어로 만들어 진다."
      ],
      "metadata": {
        "id": "6epXlBNf7I0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 정제(Cleaning)와 정규화(Normalization)"
      ],
      "metadata": {
        "id": "aJ6RKvPV-lOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"I was wondering if anyone out there could enlighten me on this car.\""
      ],
      "metadata": {
        "id": "1dCwoSDl_f12"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
        "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "shortword.sub('', text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ASIwC1Yv_nIZ",
        "outputId": "e879f1b7-193c-45bf-bf6c-9b86d7931032"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' was wondering anyone out there could enlighten this car.'"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 토큰화\n",
        "word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xor3QJCe_z2O",
        "outputId": "7ec032a0-cd28-4458-c7c0-ad7524c3885b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'was',\n",
              " 'wondering',\n",
              " 'if',\n",
              " 'anyone',\n",
              " 'out',\n",
              " 'there',\n",
              " 'could',\n",
              " 'enlighten',\n",
              " 'me',\n",
              " 'on',\n",
              " 'this',\n",
              " 'car',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 토큰화한 후 길이가 2보다 큰단어만 발췌\n",
        "[word for word in word_tokenize(text) if len(word) > 2] # 리스트 컴프리헨션으로 만듬"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2UJE8Aa_6bn",
        "outputId": "6659ef42-4bc0-407a-eec4-13339f2efd53"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['was',\n",
              " 'wondering',\n",
              " 'anyone',\n",
              " 'out',\n",
              " 'there',\n",
              " 'could',\n",
              " 'enlighten',\n",
              " 'this',\n",
              " 'car']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text = ' '.join([word for word in word_tokenize(text) if len(word) > 2])\n",
        "clean_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZB-5kIV9AH6R",
        "outputId": "5362d054-7e59-4215-f1f4-8978b8a09c21"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'was wondering anyone out there could enlighten this car'"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 어간 추출(Stemming) 및 표제어 추출(Lemmatization)"
      ],
      "metadata": {
        "id": "RpelbJOfAR3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1) 표제어 추출(Lemmatization)"
      ],
      "metadata": {
        "id": "c-Jm3pOmBWUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPy_RpoIBcI-",
        "outputId": "b09ad635-3afb-49a3-d091-711c8528c6eb"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemma = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "bmZ66wCoCDeg"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "[lemma.lemmatize(word) for word in words]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR_kn5KSCfiz",
        "outputId": "6c9a438e-d452-4219-89fb-c6ca133fd379"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['policy',\n",
              " 'doing',\n",
              " 'organization',\n",
              " 'have',\n",
              " 'going',\n",
              " 'love',\n",
              " 'life',\n",
              " 'fly',\n",
              " 'dy',\n",
              " 'watched',\n",
              " 'ha',\n",
              " 'starting']"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemma.lemmatize('doing','v'), lemma.lemmatize('dies','v'), lemma.lemmatize('watched','v'),lemma.lemmatize('lives','v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_uQhVo5CgDO",
        "outputId": "45a9ab5f-3909-4d86-c2cb-965b4107d54f"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('do', 'die', 'watch', 'live')"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemma.lemmatize('flies', 'v'), lemma.lemmatize('flies','n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjwPfldYDpbI",
        "outputId": "14e06883-0155-467e-e50c-361b622d9461"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fly', 'fly')"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemma.lemmatize('lives', 'v'), lemma.lemmatize('lives','n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMQ1cWLpDxKm",
        "outputId": "eba6f328-f2ab-4710-c2bc-038b5204506c"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('live', 'life')"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*이탤릭체 텍스트*#### 2) 어간추출"
      ],
      "metadata": {
        "id": "TGVkveC8D9id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n"
      ],
      "metadata": {
        "id": "5uavDQU9EKVa"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# djrks 추출전\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc5HDrEUER0o",
        "outputId": "5f3f717e-ff67-46fe-ee83-b3018ee79c6a"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem(\"is\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ckEAypI2Ek_G",
        "outputId": "cd8fa1e9-a57d-4468-8c10-5767845d0559"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'is'"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 어간 추출 후 \n",
        "print([ps.stem(word) for word in word_tokenize(text)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-oWUjd3EuYn",
        "outputId": "50912e1e-5172-4eaa-e829-df6770d7d7cb"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['formalize', 'allowance', 'electricical']\n",
        "print([ps.stem(word) for word in words])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW2hLdAME80u",
        "outputId": "4cff4b5c-9270-4a51-aa24-8388bdc5091a"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['formal', 'allow', 'electric']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# p\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([ps.stem(word) for word in words])\n",
        "         "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4pDFjJ5FnEX",
        "outputId": "3359ed10-70fd-4950-d445-959e6d6bd9e4"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lancaster Stemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "ls = LancasterStemmer()\n",
        "print([ls.stem(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3iFF5CsF4EF",
        "outputId": "de56296d-b80f-4e13-e182-9dded2c17875"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FXcyHJ74GTUW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}